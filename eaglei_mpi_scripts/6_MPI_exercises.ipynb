{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841f494-98d6-44a1-be26-a0550f7a0e1b",
   "metadata": {},
   "source": [
    "## Message Passing Interface (MPI) Exercises\n",
    "\n",
    "High-Performance Computing is a field that leverages parallel processing to solve complex problems efficiently. \n",
    "\n",
    "A popular approach to parallel programming is the `Message Passing Interface (MPI)`.\n",
    "\n",
    "In this tutorial, we will use Python and MPI to analyze power outage data from EAGLE-i, which contains information about the number of power outages, aggregated by county, in 15-minute time intervals. \n",
    "\n",
    "We will explore how using multiple processors to our advantage can save time when working with large datasets, and use the MPI for Python (`mpi4py`) package, which provides a simple Python interface to the MPI.\n",
    "\n",
    "MPI is designed to allows users to easily perform distributed parallel processing across multiple processors on a single computer or even multiple nodes on an HPC system.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "- The eaglei dataset is already present in the data folder. So that's all we'll be using\n",
    "- Also make sure that for just this exercise, you're using the login node jupyter server. If you've started your server in a reservation according to the instructions given to you, you can start a login node server by clicking on File->Hub Control Panel and clicking the Start button under the login node.\n",
    "\n",
    "(2014-2022 dataset available at: https://drive.google.com/drive/u/0/folders/15skV7CWnMBLkrGIPmLrh1GgNPA00iUuB)\n",
    "\n",
    "### Getting Started\n",
    "1. Load and pre-process the power outage data\n",
    "2. Divide the data into chunks to distribute among MPI processes\n",
    "3. Implement parallel processing using MPI\n",
    "4. Analyze and interpret the results\n",
    "\n",
    "### Additional Resources: \n",
    "* Commonly Used HPC Terms: https://researchcomputing.princeton.edu/learn/glossary\n",
    "* MPI Learning Resources: https://researchcomputing.princeton.edu/education/external-online-resources/mpi\n",
    "  * MPI Tutorial by LBNL: https://hpc-tutorials.llnl.gov/mpi/\n",
    "* Additional Research Computing Resources: https://researchcomputing.princeton.edu/learn/tutorials/external-resources-learning\n",
    "\n",
    "\n",
    "#### 1.  Load and pre-process the power outage data\n",
    "Let's first start by loading in a power outage data set using only one processor. The code for that is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1392c10b-8cec-47ce-8565-8322ddb9f2af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245259387\n",
      "Done with year 2014\n",
      "1565317037\n",
      "Done with year 2015\n",
      "2064484348\n",
      "Done with year 2016\n",
      "4378327281.0\n",
      "Done with year 2017\n",
      "3296135480.0\n",
      "Done with year 2018\n",
      "2824598528.0\n",
      "Done with year 2019\n",
      "5219277476.0\n",
      "Done with year 2020\n",
      "5325390318.0\n",
      "Done with year 2021\n",
      "4441543481.0\n",
      "Done with year 2022\n",
      "The total time of the script was: 64.9508786201477 seconds\n",
      "The total blackouts from 2014-2022: 29360333336.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time as tp\n",
    "\n",
    "# this line creates a variable to hold the time in which we began running the script\n",
    "t1=tp.time()\n",
    "\n",
    "# now we create a python list to hold the year of each file in the data set\n",
    "file_nums = ['2014','2015','2016','2017','2018','2019','2020','2021','2022']\n",
    "\n",
    "# we can also keep a count of the total blackouts as we read in the files using pandas\n",
    "blackouts_total = 0\n",
    "\n",
    "# this for loop iterates through each entry in our file_nums list we created above\n",
    "for i in file_nums:\n",
    "    # read the csv using pandas and assign the returned dataframe to our df variable\n",
    "    df = pd.read_csv(f'../data/eaglei_outages/eaglei_outages_{i}.csv')\n",
    "\n",
    "    # keep track of the number of blackouts in each file as we read them in\n",
    "    blackouts = df['sum']\n",
    "    blackouts_local = blackouts.sum()\n",
    "    print(blackouts_local)\n",
    "\n",
    "    blackouts_total+=blackouts_local\n",
    "\n",
    "    print('Done with year', i)\n",
    "\n",
    "# variable for the time after the script is completed so we can calculate the time the script took to run\n",
    "t2 = tp.time()\n",
    "\n",
    "print('The total time of the script was:',t2-t1, 'seconds')\n",
    "print('The total blackouts from 2014-2022:', blackouts_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf1900-7c9e-4686-b9a0-54a8a0e99c18",
   "metadata": {},
   "source": [
    "We can see that the script took a total of ~70 seconds to read in the files. \n",
    "\n",
    "#### 2. Divide the data into chunks to distribute among MPI processes\n",
    "\n",
    "Now let's try the same using MPI and multiple processors. In this example, we will use 9 processes each reading in a different file using its own processor all in parallel. To do this, we need to submit a job to Perlmutter using the sbatch script \"eagle_mpi.sbatch\"\n",
    "Now let's try the same using MPI and multiple processors. In this example, we will use 9 processes each reading in a different file using its own processor all in parallel. To do this, we need to submit a job to Perlmutter using the sbatch script \"eagle_mpi.sbatch\"\n",
    "\n",
    "\n",
    "But let's first look at the small changes we made to the code and some MPI terminology to help us understand what the changes did.\n",
    "\n",
    "Here is the MPI code:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from mpi4py import MPI\n",
    "import time as tp\n",
    "\n",
    "\n",
    "# running with 9 threads because 9 years of data, so assuming something like `mpirun -np 9 ...`\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "t1 = tp.time()\n",
    "file_nums = ['2014','2015','2016','2017','2018','2019','2020','2021','2022']\n",
    "\n",
    "df = pd.read_csv(f'../data/eaglei_outages/eaglei_outages_**{file_nums[rank]}.csv'**)\n",
    "\n",
    "blackouts = df['sum']\n",
    "blackouts_local = blackouts.sum()\n",
    "\n",
    "**blackouts_total = comm.reduce(blackouts_local, op=MPI.SUM, root=0)**\n",
    "\n",
    "t2 = tp.time()\n",
    "\n",
    "if rank==0:\n",
    "    print('The total time of the script was:',t2-t1, 'seconds')\n",
    "    print(\"Rank 0 got the sum, the total blackouts from 2014-2022:\", int(blackouts_total))\n",
    "\n",
    "```\n",
    "\n",
    "The first small change is that we are using mpi4py, which is a python package that uses MPI.\n",
    "```\n",
    "from mpi4py import MPI\n",
    "```\n",
    "To use this on your laptop or cluster, there must be a regular MPI implementation installed, like OpenMPI, in addition to the mpi4py. For the bootcamp we will use the default MPI that loaded on the Perlmutter supercomputer, which is where the Jupyer hub will send this exercise to run.\n",
    "\n",
    "\n",
    "\n",
    "# MPI Terminology\n",
    "\n",
    "To handle the parallelization, MPI sets different ranks that serve as IDs to track each of the parallel tasks. These ranks are tracked in something called a Communicator.\n",
    "\n",
    "**Communicator**   An object that represents a group of processes than can communicate with each other.\n",
    "\n",
    "**Rank** Within a communicator each process is given a unique integer ID. Ranks start at 0 and are incremented contiguously. Ranks can be mapped to hardware processing elements like CPU cores.\n",
    "\n",
    "For a typical MPI program, the number of ranks is set by the programmer in the command used to run the program. This allows the programmer to try different numbers of processes per task without needing to change the code.\n",
    "\n",
    "\n",
    "\n",
    "In the code above, the next small change is setting up the communicator and ranks with the lines:\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "```\n",
    "You can think of a communicator as a package that holds all the needed organizational information for its MPI region in the code. Inside the communicator each process is given a rank.\n",
    "\n",
    "All MPI functions within the same MPI region will get each processâ€™s rank from the communicator. Each rank has copies of the same code, so programmer must use logic, based on the MPI rank's ID to differentiate what that code processes. \n",
    "\n",
    "In the code above, the logic we use is to tie the rank number to the file number that gets opened, so that each rank is opening a different set of outage data.\n",
    "\n",
    "Look at the difference in the loop code and MPI code:\n",
    "\n",
    "Here is the loop from the serial code, which opens each file one after the other:\n",
    "```\n",
    "for i in file_nums:\n",
    "    # read the csv using pandas and assign the returned dataframe to our df variable\n",
    "    df = pd.read_csv(f'../data/eaglei_outages/eaglei_outages_{i}.csv')\n",
    "\n",
    "```\n",
    "Here is the MPI Code line that will open all the files at once, each one a different processor based on the rank ID.\n",
    "\n",
    "```\n",
    "df = pd.read_csv(f'../data/eaglei_outages/eaglei_outages_**{file_nums[rank]}.csv'**)\n",
    "\n",
    "```\n",
    "Note that the loop is gone and its index, i, has been replaced by th MPI rank ID. \n",
    "\n",
    "## Controlling the number of MPI ranks from a batch script \n",
    "\n",
    "The last piece to see is the fact that we can control the number of ranks from outside the code. This is done in the batch script that runs the code.\n",
    "\n",
    "\n",
    "A `batch script` is a file used to submit a job to a job scheduler/workload manager like Slurm. \n",
    "- The batch script contains details of the job like:\n",
    "  * the job name, \n",
    "  * max time the job can run, \n",
    "  * how many nodes are being requested for the job, \n",
    "  * which account the job should be charged to, etc.\n",
    "\n",
    "Our batch script looks like this:\n",
    "\n",
    "---\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --constraint=cpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=10\n",
    "#SBATCH --ntasks-per-node=9\n",
    "#SBATCH --job-name eaglei_mpi-%j\n",
    "#SBATCH -o eaglei_mpi-%j.out\n",
    "#SBATCH -e eaglei_mpi-%j.err\n",
    "\n",
    "module load evp-patch\n",
    "module load python\n",
    "\n",
    "srun -n9 python3 eaglei_mpi.py\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "- The `#!/bin/bash` line specifies that the script is a Bash shell script, and the commands within it will be interpreted and executed using the Bash shell.\n",
    "\n",
    "- The `#SBATCH` terms are Slurm directives, which specifies the \n",
    "\n",
    "\n",
    "\n",
    "Let's break down the components of the job execution command `srun -n9 python3 eaglei_mpi.py`:\n",
    "\n",
    "| Terms | Meaning |\n",
    "| :------- | :------- |\n",
    "| `srun` | command used to submit and execute parallel jobs on a cluster |\n",
    "| `-n9`| the `-n` flag specifies the number of processes to run in parallel, which will each execute the `eaglei_mpi.py` script independently, processing a portion of the dataset| \n",
    "|`python3` | interpreter that will be used to execute the script |\n",
    "| `eaglei_mpi.py` | python script containing the MPI code to distribute the data and tasks, that will be executed in the MPI parallel environment |\n",
    "\n",
    "#### 3. Implement parallel processing using MPI\n",
    "\n",
    "To run it, just execute the next shell which submits our script to Perlmutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37fcb6a6-478a-49ab-a813-33d2577c9188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID=!sbatch --parsable eaglei_mpi.sbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54982107-1516-4999-8912-c2f28da6ccc7",
   "metadata": {},
   "source": [
    "Once you see a new file in your directory to the left, you can open and print it with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328a54a4-b25e-4bb2-afaf-83689ff89236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time of the script was: 12.58487057685852 seconds\n",
      "Rank 0 got the sum, the total blackouts from 2014-2022: 29360333336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f\"eaglei_mpi-{ID.n}.out\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d67f25-16e5-4498-abc8-ee2207bf480b",
   "metadata": {},
   "source": [
    "We can see there was quite a speedup from sharing the load of reading in the files using MPI. \n",
    "\n",
    "Let's look at another example, in this example we will use the 2014 Eagle-I power outage data set to calculate the total number of blackouts for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83aae52f-df96-45d1-a497-d30928ee3927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of script 2.512744188308716\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>Blackout Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1037</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1051</td>\n",
       "      <td>126098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1109</td>\n",
       "      <td>4131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1121</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4017</td>\n",
       "      <td>2848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS Code  Blackout Sum\n",
       "0       1037           970\n",
       "1       1051        126098\n",
       "2       1109          4131\n",
       "3       1121           196\n",
       "4       4017          2848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tp\n",
    "\n",
    "df = pd.read_csv(f'../data/eaglei_outages/eaglei_outages_2014.csv')\n",
    "\n",
    "all_counties = df.fips_code.unique()\n",
    "\n",
    "t1=tp.time()\n",
    "counties = []\n",
    "sums = []\n",
    "\n",
    "for i in all_counties:\n",
    "    #Find the position in the CSV of a specific county (i)\n",
    "    specific_county_filter = df['fips_code'] == i\n",
    "\n",
    "    #Filter out the data so that we only look at a specific county (i)\n",
    "    specific_county_data  = df[specific_county_filter]['sum']\n",
    "\n",
    "    #Add up the total blackouts of a specific county (i)\n",
    "    specific_county_sum = specific_county_data.sum()\n",
    "    \n",
    "    counties.append(int(i))\n",
    "    sums.append(int(specific_county_sum))\n",
    "    \n",
    "\n",
    "d = {'FIPS Code': counties, 'Blackout Sum' : sums}\n",
    "blackouts_df = pd.DataFrame(data=d)  \n",
    "    \n",
    "t2=tp.time()\n",
    "\n",
    "print('Total time of script', t2-t1)\n",
    "blackouts_df.sort_values(by=['FIPS Code'])\n",
    "blackouts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938ad99-e5d0-4155-9ff0-f305f93c0e9e",
   "metadata": {},
   "source": [
    "Now, let's try the same thing but use MPI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac31c83e-7e99-4660-80a3-e3c390dd20d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID=!sbatch --parsable eaglei_mpi_ex2.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4606dee4-8a2d-4d7a-9bc3-ea63d9269d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of script 0.6176290512084961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f\"eaglei_mpi_ex2-{ID.n}.out\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "blackouts_df.sort_values(by=['FIPS Code'], ascending=True)\n",
    "blackouts_df.head()\n",
    "\n",
    "# Saves dataframe to .csv file  \n",
    "blackouts_df.to_csv('county_blackouts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e3e2-5de5-40a3-8a24-ac44e175573c",
   "metadata": {},
   "source": [
    "#### 4. Analyze and interpret the results\n",
    "To see the results, look at the column on the left and click on county_blackouts.csv.\n",
    "\n",
    "We can definitely see a speedup in our calculation using MPI to run in parallel, and we even added a step that writes our dataframe to the filesystem in .csv format so that we could load it back up here in our Jupyter notebook. Do you think we would continue to see gains in performance if we add even more parallel processes?\n",
    "\n",
    "Try editing the `my_ex2.sbatch` file and submit it with the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675d4799-b387-43a7-ae44-18e1036d041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID=!sbatch --parsable my_ex2.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fff5d25-d616-4c1a-a425-0ec758f50bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'my_ex2-sbatch: error: Invalid numeric value \"<CHANGE\" for --ntasks-per-node..out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2282290/3897807094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"my_ex2-{ID.n}.out\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mblackouts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'county_blackouts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mblackouts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FIPS Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'my_ex2-sbatch: error: Invalid numeric value \"<CHANGE\" for --ntasks-per-node..out'"
     ]
    }
   ],
   "source": [
    "with open(f\"my_ex2-{ID.n}.out\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "blackouts_df = pd.read_csv('county_blackouts.csv')\n",
    "blackouts_df.sort_values(by=['FIPS Code'], ascending=True)\n",
    "blackouts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd423e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
